base:
  - type: OS
    subtypes:

      - subtype: kmod
        data:
          ecc: true
          gdrdrv: true
          gpu_sched: true
          ib_cm: true
          ib_core: true
          ib_iser: true
          ib_uverbs: true
          iw_cm: true
          nvidia: true
          nvidia_modeset: true
          nvidia_uvm: true
          rdma_cm: true
          rdma_ucm: true
          rpcrdma: true
          sunrpc: true
          ttm: true
        context:
          ecc: GPU error detection and correction
          gdrdrv: GPUDirect RDMA support enabled
          gpu_sched: GPU workload scheduling and resource management
          ib_cm: InfiniBand connection management for RDMA
          ib_core: InfiniBand core stack for high-speed interconnect
          ib_iser: iSCSI over RDMA for fast storage access
          ib_uverbs: InfiniBand userspace verbs for direct hardware access
          iw_cm: iWARP connection management for RDMA over Ethernet
          nvidia: Core NVIDIA GPU driver
          nvidia_modeset: NVIDIA display mode setting for GPU configuration
          nvidia_uvm: Unified Virtual Memory for CPU-GPU shared memory
          rdma_cm: RDMA connection manager for efficient GPU communication
          rdma_ucm: RDMA userspace connection manager
          rpcrdma: RPC over RDMA for distributed GPU workloads
          sunrpc: RPC framework for networked GPU operations
          ttm: Translation Table Manager for GPU memory management

      - subtype: grub
        data:
          apparmor: "1"
          audit: "1"
          audit_backlog_limit: "8192"
          hugepages: "5128"
          hugepagesz: 2M
          nokaslr: ""
          security: apparmor
        context:
          apparmor: Security module (minimal GPU impact)
          audit: System auditing (minimal GPU impact)
          audit_backlog_limit: Audit queue size (minimal GPU impact)
          hugepages: Pre-allocated large memory pages for reduced TLB misses
          hugepagesz: 2MB page size for efficient GPU memory operations
          nokaslr: Deterministic kernel addressing for consistent GPU latency
          security: Security framework (minimal GPU impact)

      - subtype: sysctl
        data:
          /proc/sys/fs/aio-max-nr: "65536"
          /proc/sys/fs/file-max: "9223372036854775807"
          /proc/sys/fs/inotify/max_user_watches: "524288"
          /proc/sys/fs/nr_open: "1048576"
          /proc/sys/kernel/pid_max: "4194304"
          /proc/sys/kernel/sched_rt_period_us: "1000000"
          /proc/sys/kernel/sched_rt_runtime_us: "950000"
          /proc/sys/kernel/shmall: "18446744073692774399"
          /proc/sys/kernel/shmmax: "18446744073692774399"
          /proc/sys/kernel/threads-max: "16512444"
          /proc/sys/sunrpc/rdma_max_inline_read: "4096"
          /proc/sys/sunrpc/rdma_max_inline_write: "4096"
          /proc/sys/sunrpc/rdma_slot_table_entries: "128"
          /proc/sys/sunrpc/tcp_max_slot_table_entries: "65536"
          /proc/sys/sunrpc/transports/rdma: "1048576"
          /proc/sys/sunrpc/transports/tcp: "1048576"
          /proc/sys/vm/dirty_background_ratio: "10"
          /proc/sys/vm/dirty_ratio: "20"
          /proc/sys/vm/max_map_count: "262144"
          /proc/sys/vm/nr_hugepages: "15841"
          /proc/sys/vm/overcommit_memory: "1"
          /proc/sys/vm/swappiness: "60"
          /proc/sys/vm/zone_reclaim_mode: "0"
        context:
          /proc/sys/fs/aio-max-nr: Maximum concurrent async I/O operations for GPU data transfers
          /proc/sys/fs/file-max: System-wide file descriptor limit for GPU container connections
          /proc/sys/fs/inotify/max_user_watches: File watch limit for container monitoring and checkpointing
          /proc/sys/fs/nr_open: Per-process file descriptor limit for GPU applications
          /proc/sys/kernel/pid_max: Maximum process IDs for containerized multi-GPU workloads
          /proc/sys/kernel/sched_rt_period_us: Real-time scheduling period for GPU driver threads
          /proc/sys/kernel/sched_rt_runtime_us: Real-time CPU budget for GPU kernel operations
          /proc/sys/kernel/shmall: Total shared memory for IPC between GPU processes
          /proc/sys/kernel/shmmax: Maximum shared memory segment size for GPU data sharing
          /proc/sys/kernel/threads-max: Maximum threads for parallel GPU workloads
          /proc/sys/sunrpc/rdma_max_inline_read: RDMA inline read size for GPU-to-GPU transfers
          /proc/sys/sunrpc/rdma_max_inline_write: RDMA inline write size for efficient GPU communication
          /proc/sys/sunrpc/rdma_slot_table_entries: RDMA connection slots for multi-node GPU clusters
          /proc/sys/sunrpc/tcp_max_slot_table_entries: TCP RPC connection slots for distributed GPU training
          /proc/sys/sunrpc/transports/rdma: RDMA transport buffer size for GPUDirect transfers
          /proc/sys/sunrpc/transports/tcp: TCP transport buffer size for RPC over network
          /proc/sys/vm/dirty_background_ratio: Background writeback threshold for GPU checkpoint files
          /proc/sys/vm/dirty_ratio: Foreground writeback threshold for GPU storage operations
          /proc/sys/vm/max_map_count: Memory mapping limit for GPU memory allocations
          /proc/sys/vm/nr_hugepages: Pre-allocated 2MB hugepages for reduced TLB misses in GPU memory
          /proc/sys/vm/overcommit_memory: Always allow memory overcommit for flexible GPU container allocation
          /proc/sys/vm/swappiness: Swap tendency (lower values keep GPU-related memory resident)
          /proc/sys/vm/zone_reclaim_mode: Disable NUMA zone reclaim for consistent cross-node GPU access

  - type: SystemD
    subtypes:
      - subtype: containerd.service
        data:
          CPUAccounting: true
          CPUQuotaPerSecUSec: 18446744073709551615
          CPUSchedulingPolicy: "0"
          CPUShares: 18446744073709551615
          CPUWeight: 18446744073709551615
          Delegate: true
          DelegateControllers: '[cpu cpuacct cpuset io blkio memory devices pids bpf-firewall bpf-devices bpf-foreign bpf-socket-bind]'
          EffectiveCPUs: '[255 255 255 255 255 255 255 255 255 255 255 255 255 255 255 255 255 255 0 0 0 0 0 0]'
          EffectiveMemoryNodes: '[3 0 0 0 0 0 0 0]'
          IOSchedulingClass: "2"
          IOSchedulingPriority: "4"
          IOWeight: 18446744073709551615
          LimitNOFILE: 18446744073709551615
          LimitNPROC: 18446744073709551615
        context:
          CPUAccounting: Enables CPU usage tracking for container workloads
          CPUQuotaPerSecUSec: Unlimited CPU quota for GPU containers
          CPUSchedulingPolicy: Default scheduling policy
          CPUShares: Unlimited CPU shares
          CPUWeight: Unlimited CPU weight allows full CPU access for GPU containers
          Delegate: Allows container runtime to manage cgroups for GPU resource isolation
          DelegateControllers: Full cgroup delegation for GPU device and memory control
          EffectiveCPUs: CPU affinity mask for containerd process
          EffectiveMemoryNodes: NUMA node assignment for optimal GPU-CPU memory access
          IOSchedulingClass: Best-effort I/O scheduling class
          IOSchedulingPriority: I/O priority for container storage operations
          IOWeight: Unlimited I/O weight
          LimitNOFILE: Unlimited file descriptors for container connections
          LimitNPROC: Unlimited process count for GPU containers

  - type: K8s
    subtypes:

      - subtype: server
        data:
          version: v1.33.5
        context:
          version: Kubernetes version for GPU orchestration

      - subtype: registry
        data:
          name: nvcr.io
          repo: nvidia
          uri: nvcr.io/nvidia
        context:
          name: NVIDIA NGC container registry for GPU-optimized images
          repo: NVIDIA NGC repository
          uri: Full URI for accessing NVIDIA NGC registry

      - subtype: image
        data:
          container-toolkit: v1.17.8
          dcgm: 4.3.1-1
          dcgm-exporter: "4.3.1"
          driver: "580.82.07"
          gdrdrv: v2.5.1
          gpu-operator: v25.3.3
          gpu-operator-validator: v25.3.3
          k8s-device-plugin: v0.17.4
          k8s-dra-driver-gpu: v25.8.0
          k8s-driver-manager: v0.8.1
          k8s-mig-manager: v0.12.3
          mpi-operator: "0.6"
          node-feature-discovery: v0.17.3
          training-operator: v1
          network-operator: v25.4.0
          mofed-driver: 25.04-0.6.1.0-2
          rdma-shared-device-plugin: v1.5.3
          sriov-device-plugin: v3.9.0
          multus-cni: v4.1.0
          whereabouts-ipam: v0.7.0
          container-networking-plugins: v1.6.2
          skyhook-operator: v0.1.0
          skyhook-agent: v0.1.0
          kube-rbac-proxy: v0.18.1
          nodeos-updater-tuning: v1.0.0
        context:
          container-toolkit: NVIDIA container runtime for GPU access in containers
          dcgm: Data Center GPU Manager for GPU health monitoring and diagnostics
          dcgm-exporter: Prometheus exporter for GPU metrics and utilization
          driver: NVIDIA GPU driver for kernel-level GPU operations
          gdrdrv: GPUDirect RDMA driver for zero-copy GPU-to-NIC transfers
          gpu-operator: Orchestrates GPU stack components deployment and lifecycle
          gpu-operator-validator: Validates GPU stack configuration for optimal performance
          k8s-device-plugin: Kubernetes device plugin for GPU resource allocation and scheduling
          k8s-dra-driver-gpu: Dynamic Resource Allocation driver for advanced GPU scheduling
          k8s-driver-manager: Manages GPU driver updates and versioning
          k8s-mig-manager: Multi-Instance GPU manager for GPU partitioning and sharing
          mpi-operator: MPI job orchestration for distributed multi-GPU training
          node-feature-discovery: Discovers GPU hardware features for optimal workload placement
          training-operator: Kubernetes operator for distributed ML training on GPUs
          network-operator: NVIDIA Network Operator orchestrates Mellanox OFED and network device plugins
          mofed-driver: Mellanox OpenFabrics Enterprise Distribution for InfiniBand/RDMA
          rdma-shared-device-plugin: Kubernetes device plugin for RDMA device allocation
          sriov-device-plugin: SR-IOV device plugin for high-performance network virtualization
          multus-cni: Meta CNI plugin enabling multiple network interfaces per pod
          whereabouts-ipam: IP Address Management for secondary network interfaces
          container-networking-plugins: Standard CNI plugins for container networking
          skyhook-operator: NVIDIA Skyhook operator for automated node system tuning and optimization
          skyhook-agent: Skyhook agent that applies system configurations on GPU nodes
          kube-rbac-proxy: RBAC proxy for securing Kubernetes API access in operator deployments
          nodeos-updater-tuning: Node OS updater for applying system tuning packages (GRUB, sysctl, containerd)

      - subtype: config
        data:
          mig: false
          cdi: true
          rdma: true
          gds: false
          secure_boot: false
          vgpu: false
          use_open_kernel_module: false
          confidential_computing: false
          mig_profile: all-disabled
          mig_strategy: single
        context:
          mig: Multi-Instance GPU disabled for dedicated GPU access
          cdi: Container Device Interface enabled for GPU device injection
          rdma: RDMA support enabled for high-speed GPU networking
          gds: GPU Direct Storage enables direct GPU-to-storage transfers bypassing CPU
          secure_boot: UEFI Secure Boot requires signed GPU drivers for security compliance
          vgpu: Virtual GPU technology for GPU sharing across virtual machines
          use_open_kernel_module: Open-source NVIDIA kernel module for GPL compatibility
          confidential_computing: GPU Confidential Computing for encrypted GPU memory and secure enclaves
          mig_profile: MIG partitioning profile - all-disabled uses full GPU, numbered profiles (1g/2g/3g/4g/7g) split GPU into instances
          mig_strategy: single=uniform MIG across all GPUs, mixed=per-GPU MIG configuration for diverse workloads

      - subtype: network-config
        data:
          deploy_ofed: false
          enable_sriov: false
          use_host_mofed: false
        context:
          deploy_ofed: When true, Network Operator deploys Mellanox OFED drivers for InfiniBand/RoCE
          enable_sriov: Enables SR-IOV device plugin for high-performance network slicing
          use_host_mofed: Use existing host MOFED installation instead of containerized driver

      - subtype: skyhook-config
        data:
          runtime_required: true
          interruption_budget_percent: "100"
          tuning_interrupt_type: reboot
          manager_cpu_limit: 500m
          manager_memory_limit: 128Mi
          manager_cpu_request: 10m
          manager_memory_request: 64Mi
          node_selector: nvidia.com/gpu.product
          node_selector_values: "H100,GB200"
          toleration_key: nvidia.com/gpu
          toleration_value: "true"
        context:
          runtime_required: When true, requires tuning changes applied before workload execution
          interruption_budget_percent: Maximum percentage of nodes that can be interrupted simultaneously during updates
          tuning_interrupt_type: Type of interruption for applying configuration changes (reboot or service restart)
          manager_cpu_limit: Maximum CPU allocation for Skyhook operator manager
          manager_memory_limit: Maximum memory allocation for Skyhook operator manager
          manager_cpu_request: Requested CPU for Skyhook operator manager (guaranteed minimum)
          manager_memory_request: Requested memory for Skyhook operator manager (guaranteed minimum)
          node_selector: Kubernetes label key for selecting nodes to apply Skyhook tuning
          node_selector_values: Comma-separated values matching node_selector label (e.g., GPU models)
          toleration_key: Toleration key for scheduling on tainted GPU nodes
          toleration_value: Toleration value for matching tainted GPU nodes

  - type: GPU
    subtypes:

      - subtype: smi
        data:
          addressing-mode: ATS
          cuda-version: "13.1"
          display-active: Disabled
          driver-version: "580.82.07"
          gsp-firmware-version: "580.82.07"
          persistence-mode: Disabled
          vbios-version: 97.00.B9.00.69
        context:
          addressing-mode: Address Translation Services enables efficient GPU access to system memory via IOMMU
          cuda-version: CUDA toolkit version determining available GPU programming features and optimizations
          display-active: GPU dedicated to compute workloads without display overhead
          driver-version: Kernel driver version providing GPU hardware access and performance optimizations
          gsp-firmware-version: GPU System Processor firmware managing GPU scheduling and resource allocation
          persistence-mode: Controls GPU state retention (enabled mode reduces initialization latency)
          vbios-version: GPU BIOS firmware version determining hardware initialization and clock settings

overlays:
  - key:
      #  ?service=eks&os=ubuntu
      service: eks
      os: ubuntu
    types:
      - type: OS
        subtypes:
          - subtype: grub
            data:
              BOOT_IMAGE: /boot/vmlinuz-6.8.0-1028-aws
            context:
              BOOT_IMAGE: AWS-optimized kernel image version

  - key:
      #  ?service=eks&gpu=gb200
      service: eks
      gpu: gb200
    types:
      - type: K8s
        subtypes:
          - subtype: server
            data:
              version: v1.33.3
            context:
              version: Minimum AWS EKS Kubernetes version to support DRA for GB200 GPUs
          - subtype: image
            data:
              aws-efa-k8s-device-plugin: v0.5.3
            context:
              aws-efa-k8s-device-plugin: EFA device plugin for high-speed GPU-to-GPU networking
          - subtype: config
            data:
              use_open_kernel_module: true
            context:
              use_open_kernel_module: Use open-source EFA kernel module for compatibility with GB200 GPUs
      - type: OS
        subtypes:
          - subtype: grub
            data:
              iommu.passthrough: "1"
              init_on_alloc: "0"
              numa_balancing: disable
              default_hugepagesz: 64K
            context:
              iommu.passthrough: Bypass IOMMU translation for direct GPU memory access (GB200 requirement)
              init_on_alloc: Disabled for faster GPU memory allocation
              numa_balancing: Disabled for predictable GPU memory locality
              default_hugepagesz: 64KB hugepages optimized for GB200 memory access patterns
          - subtype: sysctl
            data:
              /proc/sys/kernel/numa_balancing: "0"
            context:
              /proc/sys/kernel/numa_balancing: Disable auto-migration for predictable GPU memory locality
          - subtype: kmod
            data:
              efa: true
            context:
              efa: AWS EFA support for GPU clusters

  - key:
      #  ?service=eks&gpu=h100
      service: eks
      gpu: h100
    types:
      - type: K8s
        subtypes:
          - subtype: server
            data:
              version: v1.30.14
            context:
              version: Validated AWS EKS Kubernetes version for H100 clusters (example snapshot)
          - subtype: image
            data:
              container-toolkit: v1.17.5
              dcgm: 4.1.1-2
              dcgm-exporter: "4.1.1"
              driver: "570.133.20"
              gdrdrv: v2.5
              gpu-operator: v25.3.0
              gpu-operator-validator: v25.3.0
              k8s-device-plugin: v0.17.1
              k8s-driver-manager: v0.8.0
              k8s-mig-manager: v0.12.1
      - type: OS
        subtypes:
          - subtype: grub
            data:
              BOOT_IMAGE: /boot/vmlinuz-6.8.0-1024-aws
            context:
              BOOT_IMAGE: AWS-optimized kernel image version (H100 example)
          - subtype: sysctl
            data:
              /proc/sys/kernel/numa_balancing: "1"
            context:
              /proc/sys/kernel/numa_balancing: Default NUMA balancing (H100 example snapshot)
      - type: GPU
        subtypes:
          - subtype: smi
            data:
              addressing-mode: HMM
              cuda-version: "12.8"
              driver-version: "570.133.20"
              gsp-firmware-version: "570.133.20"
              vbios-version: 96.00.BC.00.01