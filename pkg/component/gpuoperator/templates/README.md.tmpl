# NVIDIA GPU Operator Bundle

```shell
Generated from Cloud Native Stack Recipe  
Bundler Version: {{ .Script.Version }}
Recipe Version: {{ .Script.RecipeVersion }}
```

## Overview

This bundle contains everything needed to deploy the NVIDIA GPU Operator on your Kubernetes cluster.

## Contents

- `values.yaml` - Helm values configuration
- `manifests/clusterpolicy.yaml` - ClusterPolicy manifest
- `manifests/dcgm-exporter.yaml` - DCGM Exporter metrics ConfigMap (if enabled)
- `manifests/kernel-module-params.yaml` - Kernel module params ConfigMap (GB200 only)
- `scripts/install.sh` - Installation script
- `scripts/uninstall.sh` - Uninstallation script
- `checksums.txt` - File integrity checksums

## Prerequisites

- Kubernetes cluster (1.21+)
- Helm 3
- kubectl configured
- GPU nodes with NVIDIA GPUs

## Installation

### Option 1: Automated Installation

Run the installation script to automatically apply manifests and install the GPU Operator:

```bash
chmod +x scripts/install.sh
./scripts/install.sh
```

### Option 2: Manual Installation

Step-by-step installation for more control over the process:

```bash
# 1. Add Helm repository
helm repo add nvidia {{ .Script.HelmRepository }}
helm repo update

# 2. Create namespace
kubectl create namespace {{ .Script.Namespace }}

# 3. Apply manifests (ConfigMaps and ClusterPolicy)
kubectl apply -f manifests/ -n {{ .Script.Namespace }}

# 4. Install GPU Operator with Helm
helm upgrade --install gpu-operator nvidia/gpu-operator \
  --namespace {{ .Script.Namespace }} \
  {{- if .Script.HelmChartVersion }}
  --version {{ .Script.HelmChartVersion }} \
  {{- end }}
  --values values.yaml \
  --wait
```

## Verification

Check the GPU Operator pods:

```bash
kubectl get pods -n {{ .Script.Namespace }}
```

Verify GPU nodes:

```bash
kubectl get nodes -o json | jq '.items[].status.capacity."nvidia.com/gpu"'
```

Test GPU access in a pod:

```bash
kubectl run gpu-test --rm -it --restart=Never \
  --image=nvidia/cuda:12.0.0-base-ubuntu22.04 \
  --limits=nvidia.com/gpu=1 \
  -- nvidia-smi
```

## Configuration

### Customization

Edit `values.yaml` to customize the deployment. Key parameters:

- `driver.version` - GPU driver version
- `mig.strategy` - MIG configuration strategy
- `gds.enabled` - Enable GPUDirect Storage
- `nodeSelector` - Target specific nodes

## Uninstallation

Using the uninstallation script:

```bash
chmod +x scripts/uninstall.sh
./scripts/uninstall.sh
```

Or manually:

```bash
helm uninstall gpu-operator -n {{ .Script.Namespace }}
kubectl delete -f manifests/ --ignore-not-found=true
kubectl delete crd clusterpolicies.nvidia.com
```

## Troubleshooting

### Check GPU Operator logs

```bash
kubectl logs -n {{ .Script.Namespace }} -l app=nvidia-driver-daemonset
```

### Check ClusterPolicy status

```bash
kubectl describe clusterpolicy -n {{ .Script.Namespace }}
```

### Common Issues

1. **Driver pods not starting**: Check node compatibility
2. **CUDA version mismatch**: Verify driver version
3. **MIG configuration issues**: Review MIG strategy settings

## Support

- [GPU Operator Documentation](https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/)
- [Cloud Native Stack Repository](https://github.com/NVIDIA/cloud-native-stack)
