# NVIDIA GPU Operator Bundle

Generated from Cloud Native Stack Recipe  
Timestamp: {{ .Script.Timestamp }}
Bundler Version: {{ .Script.Version }}
Recipe Bundler Version: {{ .Script.RecipeVersion }}

## Overview

This bundle contains everything needed to deploy the NVIDIA GPU Operator on your Kubernetes cluster.

## Contents

- `values.yaml` - Helm values configuration
- `clusterpolicy.yaml` - ClusterPolicy manifest
- `scripts/install.sh` - Installation script
- `scripts/uninstall.sh` - Uninstallation script
- `checksums.txt` - File integrity checksums

## Prerequisites

- Kubernetes cluster ({{ if .Script.K8sVersion }}{{ .Script.K8sVersion }}{{ else }}1.21+{{ end }})
- Helm 3
- kubectl configured
{{- if .Script.GPUType }}
- GPU nodes with {{ .Script.GPUType }} GPUs
{{- end }}

## Installation

### Option 1: Using the Installation Script

```bash
chmod +x scripts/install.sh
./scripts/install.sh
```

### Option 2: Manual Installation with Helm

```bash
# Add Helm repository
helm repo add nvidia {{ .Script.HelmRepository }}
helm repo update

# Create namespace
kubectl create namespace {{ .Helm.Namespace }}

# Install GPU Operator
helm upgrade --install gpu-operator nvidia/gpu-operator \
  --namespace {{ .Helm.Namespace }} \
  {{- if .Script.HelmChartVersion }}
  --version {{ .Script.HelmChartVersion }} \
  {{- end }}
  --values values.yaml \
  --wait
```

### Option 3: Using kubectl

```bash
kubectl apply -f clusterpolicy.yaml
```

## Verification

Check the GPU Operator pods:

```bash
kubectl get pods -n {{ .Helm.Namespace }}
```

Verify GPU nodes:

```bash
kubectl get nodes -o json | jq '.items[].status.capacity."nvidia.com/gpu"'
```

Test GPU access in a pod:

```bash
kubectl run gpu-test --rm -it --restart=Never \
  --image=nvidia/cuda:12.0.0-base-ubuntu22.04 \
  --limits=nvidia.com/gpu=1 \
  -- nvidia-smi
```

## Configuration

### Key Settings

{{- if .Script.DriverVersion }}
- **Driver Version**: {{ .Script.DriverVersion }}
{{- end }}
{{- if .Script.MIGStrategy }}
- **MIG Strategy**: {{ .Script.MIGStrategy }}
{{- end }}
{{- if .Script.EnableGDS }}
- **GPUDirect Storage**: Enabled
{{- end }}
{{- if .Script.EnableCDI }}
- **CDI**: Enabled
{{- end }}

### Customization

Edit `values.yaml` to customize the deployment. Key parameters:

- `driver.version` - GPU driver version
- `mig.strategy` - MIG configuration strategy
- `gds.enabled` - Enable GPUDirect Storage
- `nodeSelector` - Target specific nodes

## Uninstallation

Using the uninstallation script:

```bash
chmod +x scripts/uninstall.sh
./scripts/uninstall.sh
```

Or manually:

```bash
helm uninstall gpu-operator -n {{ .Helm.Namespace }}
kubectl delete crd clusterpolicies.nvidia.com
```

## Troubleshooting

### Check GPU Operator logs

```bash
kubectl logs -n {{ .Helm.Namespace }} -l app=nvidia-driver-daemonset
```

### Check ClusterPolicy status

```bash
kubectl describe clusterpolicy -n {{ .Helm.Namespace }}
```

### Common Issues

1. **Driver pods not starting**: Check node compatibility
2. **CUDA version mismatch**: Verify driver version
3. **MIG configuration issues**: Review MIG strategy settings

## Support

- [GPU Operator Documentation](https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/)
- [Cloud Native Stack Repository](https://github.com/NVIDIA/cloud-native-stack)

## Generated Configuration

{{- if .Script.Request }}
**Recipe Query:**
- OS: {{ .Script.Request.Os }}
{{- if .Script.Request.OsVersion }}
- OS Version: {{ .Script.Request.OsVersion }}
{{- end }}
{{- if .Script.Request.Service }}
- Service: {{ .Script.Request.Service }}
{{- end }}
{{- if .Script.Request.GPU }}
- GPU: {{ .Script.Request.GPU }}
{{- end }}
{{- if .Script.Request.Intent }}
- Intent: {{ .Script.Request.Intent }}
{{- end }}
{{- end }}
