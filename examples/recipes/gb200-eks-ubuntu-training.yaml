kind: Recipe
apiVersion: recipe.dgxc.io/v1
metadata:
  recipe-timestamp: "2026-01-02T17:45:46Z"
  recipe-version: 0.8.12
request:
  os: ubuntu
  osv:
    major: 24
    minor: 4
    precision: 2
  kernel:
    major: 6
    minor: 8
    precision: 3
    extras: -1028-aws
  service: eks
  k8s:
    major: 1
    minor: 33
    patch: 5
    precision: 3
    extras: -eks-3025e55
  gpu: gb200
  intent: training
  withContext: true
matchedRules:
  - 'OS: ubuntu any, Kernel: any, Service: eks, K8s: any, GPU: any, Intent: any, Context: false'
  - 'OS: any any, Kernel: any, Service: eks, K8s: any, GPU: gb200, Intent: any, Context: false'
measurements:
  - type: OS
    subtypes:
      - subtype: kmod
        data:
          ecc: true
          efa: true
          gdrdrv: true
          gpu_sched: true
          ib_cm: true
          ib_core: true
          ib_iser: true
          ib_uverbs: true
          iw_cm: true
          nvidia: true
          nvidia_modeset: true
          nvidia_uvm: true
          rdma_cm: true
          rdma_ucm: true
          rpcrdma: true
          sunrpc: true
          ttm: true
        context:
          ecc: GPU error detection and correction
          efa: AWS EFA support for GPU clusters
          gdrdrv: GPUDirect RDMA support enabled
          gpu_sched: GPU workload scheduling and resource management
          ib_cm: InfiniBand connection management for RDMA
          ib_core: InfiniBand core stack for high-speed interconnect
          ib_iser: iSCSI over RDMA for fast storage access
          ib_uverbs: InfiniBand userspace verbs for direct hardware access
          iw_cm: iWARP connection management for RDMA over Ethernet
          nvidia: Core NVIDIA GPU driver
          nvidia_modeset: NVIDIA display mode setting for GPU configuration
          nvidia_uvm: Unified Virtual Memory for CPU-GPU shared memory
          rdma_cm: RDMA connection manager for efficient GPU communication
          rdma_ucm: RDMA userspace connection manager
          rpcrdma: RPC over RDMA for distributed GPU workloads
          sunrpc: RPC framework for networked GPU operations
          ttm: Translation Table Manager for GPU memory management
      - subtype: grub
        data:
          BOOT_IMAGE: /boot/vmlinuz-6.8.0-1028-aws
          apparmor: "1"
          audit: "1"
          audit_backlog_limit: "8192"
          default_hugepagesz: 64K
          hugepages: "5128"
          hugepagesz: 2M
          init_on_alloc: "0"
          iommu.passthrough: "1"
          nokaslr: ""
          numa_balancing: disable
          security: apparmor
        context:
          BOOT_IMAGE: AWS-optimized kernel image version
          apparmor: Security module (minimal GPU impact)
          audit: System auditing (minimal GPU impact)
          audit_backlog_limit: Audit queue size (minimal GPU impact)
          default_hugepagesz: 64KB hugepages optimized for GB200 memory access patterns
          hugepages: Pre-allocated large memory pages for reduced TLB misses
          hugepagesz: 2MB page size for efficient GPU memory operations
          init_on_alloc: Disabled for faster GPU memory allocation
          iommu.passthrough: Bypass IOMMU translation for direct GPU memory access (GB200 requirement)
          nokaslr: Deterministic kernel addressing for consistent GPU latency
          numa_balancing: Disabled for predictable GPU memory locality
          security: Security framework (minimal GPU impact)
      - subtype: sysctl
        data:
          /proc/sys/fs/aio-max-nr: "65536"
          /proc/sys/fs/file-max: "9223372036854775807"
          /proc/sys/fs/inotify/max_user_watches: "524288"
          /proc/sys/fs/nr_open: "1048576"
          /proc/sys/kernel/numa_balancing: "0"
          /proc/sys/kernel/pid_max: "4194304"
          /proc/sys/kernel/sched_rt_period_us: "1000000"
          /proc/sys/kernel/sched_rt_runtime_us: "950000"
          /proc/sys/kernel/shmall: "18446744073692774399"
          /proc/sys/kernel/shmmax: "18446744073692774399"
          /proc/sys/kernel/threads-max: "16512444"
          /proc/sys/sunrpc/rdma_max_inline_read: "4096"
          /proc/sys/sunrpc/rdma_max_inline_write: "4096"
          /proc/sys/sunrpc/rdma_slot_table_entries: "128"
          /proc/sys/sunrpc/tcp_max_slot_table_entries: "65536"
          /proc/sys/sunrpc/transports/rdma: "1048576"
          /proc/sys/sunrpc/transports/tcp: "1048576"
          /proc/sys/vm/dirty_background_ratio: "10"
          /proc/sys/vm/dirty_ratio: "20"
          /proc/sys/vm/max_map_count: "262144"
          /proc/sys/vm/nr_hugepages: "15841"
          /proc/sys/vm/overcommit_memory: "1"
          /proc/sys/vm/swappiness: "60"
          /proc/sys/vm/zone_reclaim_mode: "0"
        context:
          /proc/sys/fs/aio-max-nr: Maximum concurrent async I/O operations for GPU data transfers
          /proc/sys/fs/file-max: System-wide file descriptor limit for GPU container connections
          /proc/sys/fs/inotify/max_user_watches: File watch limit for container monitoring and checkpointing
          /proc/sys/fs/nr_open: Per-process file descriptor limit for GPU applications
          /proc/sys/kernel/numa_balancing: Disable auto-migration for predictable GPU memory locality
          /proc/sys/kernel/pid_max: Maximum process IDs for containerized multi-GPU workloads
          /proc/sys/kernel/sched_rt_period_us: Real-time scheduling period for GPU driver threads
          /proc/sys/kernel/sched_rt_runtime_us: Real-time CPU budget for GPU kernel operations
          /proc/sys/kernel/shmall: Total shared memory for IPC between GPU processes
          /proc/sys/kernel/shmmax: Maximum shared memory segment size for GPU data sharing
          /proc/sys/kernel/threads-max: Maximum threads for parallel GPU workloads
          /proc/sys/sunrpc/rdma_max_inline_read: RDMA inline read size for GPU-to-GPU transfers
          /proc/sys/sunrpc/rdma_max_inline_write: RDMA inline write size for efficient GPU communication
          /proc/sys/sunrpc/rdma_slot_table_entries: RDMA connection slots for multi-node GPU clusters
          /proc/sys/sunrpc/tcp_max_slot_table_entries: TCP RPC connection slots for distributed GPU training
          /proc/sys/sunrpc/transports/rdma: RDMA transport buffer size for GPUDirect transfers
          /proc/sys/sunrpc/transports/tcp: TCP transport buffer size for RPC over network
          /proc/sys/vm/dirty_background_ratio: Background writeback threshold for GPU checkpoint files
          /proc/sys/vm/dirty_ratio: Foreground writeback threshold for GPU storage operations
          /proc/sys/vm/max_map_count: Memory mapping limit for GPU memory allocations
          /proc/sys/vm/nr_hugepages: Pre-allocated 2MB hugepages for reduced TLB misses in GPU memory
          /proc/sys/vm/overcommit_memory: Always allow memory overcommit for flexible GPU container allocation
          /proc/sys/vm/swappiness: Swap tendency (lower values keep GPU-related memory resident)
          /proc/sys/vm/zone_reclaim_mode: Disable NUMA zone reclaim for consistent cross-node GPU access
  - type: SystemD
    subtypes:
      - subtype: containerd.service
        data:
          CPUAccounting: true
          CPUQuotaPerSecUSec: 18446744073709551615
          CPUSchedulingPolicy: "0"
          CPUShares: 18446744073709551615
          CPUWeight: 18446744073709551615
          Delegate: true
          DelegateControllers: '[cpu cpuacct cpuset io blkio memory devices pids bpf-firewall bpf-devices bpf-foreign bpf-socket-bind]'
          EffectiveCPUs: '[255 255 255 255 255 255 255 255 255 255 255 255 255 255 255 255 255 255 0 0 0 0 0 0]'
          EffectiveMemoryNodes: '[3 0 0 0 0 0 0 0]'
          IOSchedulingClass: "2"
          IOSchedulingPriority: "4"
          IOWeight: 18446744073709551615
          LimitNOFILE: 18446744073709551615
          LimitNPROC: 18446744073709551615
        context:
          CPUAccounting: Enables CPU usage tracking for container workloads
          CPUQuotaPerSecUSec: Unlimited CPU quota for GPU containers
          CPUSchedulingPolicy: Default scheduling policy
          CPUShares: Unlimited CPU shares
          CPUWeight: Unlimited CPU weight allows full CPU access for GPU containers
          Delegate: Allows container runtime to manage cgroups for GPU resource isolation
          DelegateControllers: Full cgroup delegation for GPU device and memory control
          EffectiveCPUs: CPU affinity mask for containerd process
          EffectiveMemoryNodes: NUMA node assignment for optimal GPU-CPU memory access
          IOSchedulingClass: Best-effort I/O scheduling class
          IOSchedulingPriority: I/O priority for container storage operations
          IOWeight: Unlimited I/O weight
          LimitNOFILE: Unlimited file descriptors for container connections
          LimitNPROC: Unlimited process count for GPU containers
  - type: K8s
    subtypes:
      - subtype: server
        data:
          version: v1.33.3
        context:
          version: Minimum AWS EKS Kubernetes version to support DRA for GB200 GPUs
      - subtype: registry
        data:
          name: nvcr.io
          repo: nvidia
          uri: nvcr.io/nvidia
        context:
          name: NVIDIA NGC container registry for GPU-optimized images
          repo: NVIDIA NGC repository
          uri: Full URI for accessing NVIDIA NGC registry
      - subtype: image
        data:
          aws-efa-k8s-device-plugin: v0.5.3
          container-networking-plugins: v1.6.2
          container-toolkit: v1.17.8
          dcgm: 4.3.1-1
          dcgm-exporter: 4.3.1
          driver: 580.82.07
          gdrdrv: v2.5.1
          gpu-operator: v25.3.3
          gpu-operator-validator: v25.3.3
          k8s-device-plugin: v0.17.4
          k8s-dra-driver-gpu: v25.8.0
          k8s-driver-manager: v0.8.1
          k8s-mig-manager: v0.12.3
          mofed-driver: 25.04-0.6.1.0-2
          mpi-operator: "0.6"
          multus-cni: v4.1.0
          network-operator: v25.4.0
          node-feature-discovery: v0.17.3
          rdma-shared-device-plugin: v1.5.3
          sriov-device-plugin: v3.9.0
          training-operator: v1
          whereabouts-ipam: v0.7.0
        context:
          aws-efa-k8s-device-plugin: EFA device plugin for high-speed GPU-to-GPU networking
          container-networking-plugins: Standard CNI plugins for container networking
          container-toolkit: NVIDIA container runtime for GPU access in containers
          dcgm: Data Center GPU Manager for GPU health monitoring and diagnostics
          dcgm-exporter: Prometheus exporter for GPU metrics and utilization
          driver: NVIDIA GPU driver for kernel-level GPU operations
          gdrdrv: GPUDirect RDMA driver for zero-copy GPU-to-NIC transfers
          gpu-operator: Orchestrates GPU stack components deployment and lifecycle
          gpu-operator-validator: Validates GPU stack configuration for optimal performance
          k8s-device-plugin: Kubernetes device plugin for GPU resource allocation and scheduling
          k8s-dra-driver-gpu: Dynamic Resource Allocation driver for advanced GPU scheduling
          k8s-driver-manager: Manages GPU driver updates and versioning
          k8s-mig-manager: Multi-Instance GPU manager for GPU partitioning and sharing
          mofed-driver: Mellanox OpenFabrics Enterprise Distribution for InfiniBand/RDMA
          mpi-operator: MPI job orchestration for distributed multi-GPU training
          multus-cni: Meta CNI plugin enabling multiple network interfaces per pod
          network-operator: NVIDIA Network Operator orchestrates Mellanox OFED and network device plugins
          node-feature-discovery: Discovers GPU hardware features for optimal workload placement
          rdma-shared-device-plugin: Kubernetes device plugin for RDMA device allocation
          sriov-device-plugin: SR-IOV device plugin for high-performance network virtualization
          training-operator: Kubernetes operator for distributed ML training on GPUs
          whereabouts-ipam: IP Address Management for secondary network interfaces
      - subtype: config
        data:
          cdi: true
          confidential_computing: false
          gds: false
          mig: false
          mig_profile: all-disabled
          mig_strategy: single
          rdma: true
          secure_boot: false
          use_open_kernel_module: true
          vgpu: false
        context:
          cdi: Container Device Interface enabled for GPU device injection
          confidential_computing: GPU Confidential Computing for encrypted GPU memory and secure enclaves
          gds: GPU Direct Storage enables direct GPU-to-storage transfers bypassing CPU
          mig: Multi-Instance GPU disabled for dedicated GPU access
          mig_profile: MIG partitioning profile - all-disabled uses full GPU, numbered profiles (1g/2g/3g/4g/7g) split GPU into instances
          mig_strategy: single=uniform MIG across all GPUs, mixed=per-GPU MIG configuration for diverse workloads
          rdma: RDMA support enabled for high-speed GPU networking
          secure_boot: UEFI Secure Boot requires signed GPU drivers for security compliance
          use_open_kernel_module: Use open-source EFA kernel module for compatibility with GB200 GPUs
          vgpu: Virtual GPU technology for GPU sharing across virtual machines
      - subtype: network-config
        data:
          deploy_ofed: false
          enable_sriov: false
          use_host_mofed: false
        context:
          deploy_ofed: When true, Network Operator deploys Mellanox OFED drivers for InfiniBand/RoCE
          enable_sriov: Enables SR-IOV device plugin for high-performance network slicing
          use_host_mofed: Use existing host MOFED installation instead of containerized driver
  - type: GPU
    subtypes:
      - subtype: smi
        data:
          addressing-mode: ATS
          cuda-version: "13.1"
          display-active: Disabled
          driver-version: 580.82.07
          gsp-firmware-version: 580.82.07
          persistence-mode: Disabled
          vbios-version: 97.00.B9.00.69
        context:
          addressing-mode: Address Translation Services enables efficient GPU access to system memory via IOMMU
          cuda-version: CUDA toolkit version determining available GPU programming features and optimizations
          display-active: GPU dedicated to compute workloads without display overhead
          driver-version: Kernel driver version providing GPU hardware access and performance optimizations
          gsp-firmware-version: GPU System Processor firmware managing GPU scheduling and resource allocation
          persistence-mode: Controls GPU state retention (enabled mode reduces initialization latency)
          vbios-version: GPU BIOS firmware version determining hardware initialization and clock settings
